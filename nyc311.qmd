---
title: 'Stat 3255 Midterm Project: Analysis of NYC Open Data of 311 Service Requests'
author: 'Ethan Long'
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    documentclass: article
    number-sections: true
    toc: true
    geometry:
      - top=30mm
      - left=20mm
    html:
      code-fold: true
      html-math-method: katex
---

# Introduction

The midterm project of Stat-3255 tasks us with telling a story of NYC Open Data
311 Service Requests. More specficially Street Flooding (SF) and Catch Basin
(CB) complaints. SF complaints indicates instances of street flooding, while BC
complaints indicate instances of catch basin drains failing to properly drain. 
This could be caused by several reasons, most commonly due to drain blockages.



# Data Cleaning 

To start we must properly import our data file and clean where necessary. 

Import Data
```{python}

# Import necessary packages
import pandas as pd
import numpy as np
import os 

# Initialize file path to variable 'file'
csv_file = 'nycflood2024.csv'

# Read csv file
nyc_flood = pd.read_csv(csv_file)

# Print first 5 rows, get familiar with dataframe
print(nyc_flood.head())

# Take all column names and properly format
nyc_flood.columns = nyc_flood.columns.str.replace(' ','_').str.lower()

# Summarize missing data in columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

# Convert data into arrow format
feather_file = 'nyc_flood_feather.feather'
nyc_flood.to_feather(feather_file)
nyc_flood_feather = pd.read_feather(feather_file)

# See difference in sizes between feather and csv files
print(f'CSV file size: {os.path.getsize(csv_file) / (1024 * 1024)} MB')
print(f'Feather file size: {os.path.getsize(feather_file) / (1024 * 1024)} MB')

```

## Finding and Dropping Redundant Columns

```{python}

# Find redundant columns

# Noticed how the first five entries where of the same agency
# Tested to see if the entire column is the 'Department of 
# Environmental Protection'

print(nyc_flood_feather['agency_name'].value_counts())
# Every entry is identical, no need to keep the column

# Identify any empty columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

# The following columns are completely empty: 
# location_type, landmark, facility_type, due_date, vehicle_type,
#  taxi_company_borough, taxi_pick_up_location, 
# bridge_highway_name, bridge_highway_direction, road_ramp, 
# bridge_highway_segment

# From my prior experience with NYC Open Data, the 'location' is 
# redundant as it contains longitude and latitude coordinates which
#  have already been given

# Drop all columns deemed unnecessary
nyc_flood_feather.drop(columns=['agency_name', 'location_type', 'landmark', 'facility_type', 'due_date', 'vehicle_type', 'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment'], inplace=True)

# Get summary of new data
print(nyc_flood_feather.head())

```

## Finding Invalid Values of NYC Zipcodes or Boroughs

### Familiarize With the Two Columns in Question

```{python}

# Familiarize myself with the values in both columns 
print(nyc_flood_feather['incident_zip'].head())
print(nyc_flood_feather['borough'].head())

# Find the frequency of values appearing in borough column
print(nyc_flood_feather['borough'].value_counts())

# Find the amount of incidents missing zip
print(nyc_flood_feather['borough'].value_counts())


``` 


### Drop the Rows in the Two Columns With Messy Entries


```{python}
# Borough data looks clean besides 2 rows missing
# Zip data looks clean as only 4 are missing entries
# Extremely small sample, we can just drop the 6 total rows
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['borough'] != 'Unspecified']
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].notnull()]

# Double check that the two columns are clean
print(nyc_flood_feather['borough'].value_counts())
print(nyc_flood_feather['borough'].value_counts())

```

### Check Zip Codes For Their Validity

```{python}

# Get all valid nyc zip codes
valid_nyc_zcs = [
    10001, 10002, 10003, 10004, 10005, 10006, 10007, 10009, 10010, 10011, 
    10012, 10013, 10014, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 
    10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 
    10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10044, 10128, 
    10280, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 
    10310, 10312, 10314, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 
    10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 
    10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 11004, 11005, 
    11101, 11102, 11103, 11104, 11105, 11106, 11201, 11203, 11204, 11205, 
    11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 
    11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 
    11226, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 
    11237, 11238, 11239, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 
    11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 
    11372, 11373, 11374, 11375, 11377, 11378, 11379, 11385, 11411, 11412, 
    11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 
    11423, 11426, 11427, 11428, 11429, 11432, 11433, 11434, 11435, 11436, 
    11691, 11692, 11693, 11694, 11695, 11697
]

initial_len = len(nyc_flood_feather)

print(f'Rows before checking validity of zips: {len(nyc_flood_feather)}')

nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].isin(valid_nyc_zcs)]

post_len = len(nyc_flood_feather)

print(f'Rows after checking validity of zips: {len(nyc_flood_feather)}')

print(f' There were {initial_len - post_len} rows dropped')

```

## Checking for Possible Date Errors

```{python}

# Convert the columns to datetime format for efficiency
nyc_flood_feather['closed_date'] = pd.to_datetime(nyc_flood_feather['closed_date'])
nyc_flood_feather['created_date'] = pd.to_datetime(nyc_flood_feather['created_date'])

# Since we are comparing times to noon & midnight, lets define the two times
noon = pd.to_datetime('00:00:00').time()
midnight = pd.to_datetime('12:00:00').time()

# First check rows where closed_date is earlier than created_date
dating_error_count = (
    (nyc_flood_feather['closed_date'] < nyc_flood_feather['created_date']) | 
    # Then check rows where the times are exactly the same
    (nyc_flood_feather['closed_date'] == nyc_flood_feather['created_date']) |
    ( # Then find when the times occur exactly at midnight
        (nyc_flood_feather['closed_date'].dt.time == midnight) & 
        (nyc_flood_feather['created_date'].dt.time == midnight)
    ) | 
    ( # Then find when the times occur exactly at noon
        (nyc_flood_feather['closed_date'].dt.time == noon) & 
        (nyc_flood_feather['created_date'].dt.time == noon)
    )
).sum()

print(f'Number of dating errors found in the dataset: {dating_error_count}')

```

## Dropping Rows Featuring Date Errors

Since there are 159 rows with dating errors, I believe that the best course of 
action is to drop the rows. The dataset is over 9,000 rows long and I don't
believe dropping %1.7 of the dataset will significantly impact anything.
