---
title: 'Stat 3255 Midterm Project: Analysis of NYC Open Data of 311 Service Requests'
author: 'Ethan Long'
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    documentclass: article
    number-sections: true
    toc: true
    geometry:
      - top=30mm
      - left=20mm
    html:
      code-fold: true
      html-math-method: katex
---

# Introduction

The midterm project of Stat-3255 tasks us with telling a story with NYC Open 
Data 311 Service Requests. More specficially Street Flooding (SF) and Catch 
Basin (CB) complaints. SF complaints indicates instances of street flooding, 
while BC complaints indicate instances of catch basin drains failing to 
properly drain. This could be caused by several reasons, most commonly due to 
drain blockages.



# Data Cleaning 


## Importing File and Getting Familiar With Dataset

To start we must properly import our data file and clean where necessary.
We will do this in several steps. 

Import Data
```{python}

# Import necessary packages
import pandas as pd
import numpy as np
import os 

# Initialize file path to variable 'file'
csv_file = 'nycflood2024.csv'

# Read csv file
nyc_flood = pd.read_csv(csv_file)

# Print first 5 rows, get familiar with dataframe
print(nyc_flood.head())

# Take all column names and properly format
nyc_flood.columns = nyc_flood.columns.str.replace(' ','_').str.lower()

# Summarize missing data in columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

```


##  Converting File Type in Arrow Format for Storage Optimization

We have imported a rather large .csv file and I wish to convert it into a more
efficient file type. Arrow provides us with exactly that, so I will create a
new filepath and convert the .csv file into feather format. I will show the 
amount of space we will save while using the feather file instead of the .csv.

```{python}

# Convert data into arrow format
feather_file = 'nyc_flood_feather.feather'
nyc_flood.to_feather(feather_file)
nyc_flood_feather = pd.read_feather(feather_file)

# See difference in sizes between feather and csv files
print(f'CSV file size: {os.path.getsize(csv_file) / (1024 * 1024)} MB')
print(f'Feather file size: {os.path.getsize(feather_file) / (1024 * 1024)} MB')

```


## Finding and Dropping Redundant Columns

At first glance of the dataframe preview, we can see countless columns with 
either suspicous amounts of missing data or redundant natures. We will explore
numerous columns and see if they are needed for our purposes. 

```{python}

# Find redundant columns

# Noticed how the first five entries in agency_name and complaint 
# type are identical. Tested to see if the entire column is the 'Department of 
# Environmental Protection' and 'Sewer' as complaint type


print(nyc_flood_feather['agency_name'].value_counts())
print(nyc_flood_feather['complaint_type'].value_counts())
# Every entry is identical, no need to keep the columns

# Identify any empty columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

# The following columns are completely empty: 
# location_type, landmark, facility_type, due_date, vehicle_type,
# taxi_company_borough, taxi_pick_up_location, 
# bridge_highway_name, bridge_highway_direction, road_ramp, 
# bridge_highway_segment

# From my prior experience with NYC Open Data, the 'location' is 
# redundant as it contains longitude and latitude coordinates which
# has already been given

# Drop all columns deemed unnecessary
nyc_flood_feather.drop(columns=['agency_name', 'complaint_type', 'location_type', 'landmark', 'facility_type', 'due_date', 'vehicle_type', 'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment'], inplace=True)

# Get summary of new data
print(nyc_flood_feather.head())

```


## Finding Invalid Values of NYC Zipcodes or Boroughs


### Familiarize With the Two Columns in Question

If we are to validate and clean these two columns, we must get comfortable with
both's data and nature. Lets take a preview of the two columns and get a rough
idea what kind of data these columns house.

```{python}

# Familiarize myself with the values in both columns 
print(nyc_flood_feather['incident_zip'].head())
print(nyc_flood_feather['borough'].head())

# Find the frequency of values appearing in borough column
print(nyc_flood_feather['borough'].value_counts())

# Find the amount of incidents missing zip
print(nyc_flood_feather['borough'].value_counts())

```


### Drop the Rows in the Two Columns With Messy Entries

Using our recently gained insights on the contents of both the borough and 
zip code columns, we will drop the rows that contain data we don't want. 

```{python}

# Borough column looks clean besides 2 rows missing
# Zip data looks clean as only 4 are missing entries
# Extremely small sample, we can just drop the 6 total rows
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['borough'] != 'Unspecified']
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].notnull()]

# Double check that the two columns are clean
print(nyc_flood_feather['borough'].value_counts())
print(nyc_flood_feather['incident_zip'].isnull().sum())

```


### Check Zip Codes For Their Validity

We need to check each zip code's validity as our prior experience with NYC's 
Open Data has exposed a slight tendancy to insert invalid zip codes throughout
the data. This most likely occurs through human error and is relatively 
harmless. But as I anticipate myself heavily using zip code data later in this
project, I will drop any row housing an invalid NYC zip code. 


```{python}

# Get all valid nyc zip codes
valid_nyc_zcs = [
    10001, 10002, 10003, 10004, 10005, 10006, 10007, 10009, 10010, 10011, 
    10012, 10013, 10014, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 
    10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 
    10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10044, 10128, 
    10280, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 
    10310, 10312, 10314, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 
    10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 
    10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 11004, 11005, 
    11101, 11102, 11103, 11104, 11105, 11106, 11201, 11203, 11204, 11205, 
    11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 
    11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 
    11226, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 
    11237, 11238, 11239, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 
    11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 
    11372, 11373, 11374, 11375, 11377, 11378, 11379, 11385, 11411, 11412, 
    11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 
    11423, 11426, 11427, 11428, 11429, 11432, 11433, 11434, 11435, 11436, 
    11691, 11692, 11693, 11694, 11695, 11697
]

# Set value of initial list length
initial_len = len(nyc_flood_feather)
print(f'Rows before checking validity of zips: {len(nyc_flood_feather)}')

# Set dataframe to itself but omit all rows with missing zips
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].isin(valid_nyc_zcs)]

# Set value of post-parsing list length
post_len = len(nyc_flood_feather)
print(f'Rows after checking validity of zips: {len(nyc_flood_feather)}')

# Show the amount of rows dropped
print(f' There were {initial_len - post_len} rows dropped')

```

## Checking for Possible Dating Errors

The possible dating errors we need to check for in this dataset are as follows:

1. If the 'closed_date' occurs before the 'created_date'
1. If the 'closed_date' occurs at the same time as 'created_date'
1. If the two dates occur at exactly midnight
1. If the two dates occur at exactly noon

```{python}

# Convert the columns to datetime format for efficiency
nyc_flood_feather['closed_date'] = pd.to_datetime(nyc_flood_feather['closed_date'])
nyc_flood_feather['created_date'] = pd.to_datetime(nyc_flood_feather['created_date'])

# Since we are comparing times to noon & midnight, lets define the two times
noon = pd.to_datetime('00:00:00').time()
midnight = pd.to_datetime('12:00:00').time()

# First check rows where closed_date is earlier than created_date
dating_error_count = (
    (nyc_flood_feather['closed_date'] < nyc_flood_feather['created_date']) | 
    # Then check rows where the times are exactly the same
    (nyc_flood_feather['closed_date'] == nyc_flood_feather['created_date']) |
    ( # Then find when the times occur exactly at midnight
        (nyc_flood_feather['closed_date'].dt.time == midnight) & 
        (nyc_flood_feather['created_date'].dt.time == midnight)
    ) | 
    ( # Then find when the times occur exactly at noon
        (nyc_flood_feather['closed_date'].dt.time == noon) & 
        (nyc_flood_feather['created_date'].dt.time == noon)
    )
).sum()

print(f'Number of dating errors found in the dataset: {dating_error_count}')

```

### Dropping Rows Featuring Date Errors

Since there are 159 rows with dating errors, I believe that the best course of 
action is to drop the rows. The dataset spans over 9,000 rows long and I don't
believe dropping 1.7% of the dataset will significantly impact our findings.

```{python}

rows_to_remove = (
      (nyc_flood_feather['closed_date'] < nyc_flood_feather['created_date']) | 
    # Then check rows where the times are exactly the same
    (nyc_flood_feather['closed_date'] == nyc_flood_feather['created_date']) |
    ( # Then find when the times occur exactly at midnight
        (nyc_flood_feather['closed_date'].dt.time == midnight) & 
        (nyc_flood_feather['created_date'].dt.time == midnight)
    ) | 
    ( # Then find when the times occur exactly at noon
        (nyc_flood_feather['closed_date'].dt.time == noon) & 
        (nyc_flood_feather['created_date'].dt.time == noon)
    )
)

nyc_flood_feather = nyc_flood_feather[~rows_to_remove]
print(f' Dataset length before data cleaning: {len(nyc_flood)}')
print(f' Dataset length after data cleaning: {len(nyc_flood_feather)}')

```

## Suggestions to the Data Curator 

From my experience working with the 311 Service Request dataset, I can conclude
that the dataset is a thoughtful, well-rounded dataset that has no major flaws. 
But I believe there are minor tweaks that can be made to improve and optimize
the dataset. To begin there are several columns that are completely empty. 
I don't know whether or not these columns are useful, but I do know that 
they are completely empty. They take up valuable storage space and could 
unneccesarily slow down computing speed. There are 11 of these columns that are
completely empty and therefore obsolete. Secondly there are two columns that 
feature the same entry for all ~9,000 rows. Another example of columns taking 
up storage space and not offering any valuable insights. These two rows,
agency_name & complaint_type, should be omitted in future datasets, unless they
offer variation. Lastly, there are a couple of instances where rows feature
invalid NYC zip codes, empty borough entries, and dating errors. There are only
around ~150 instance of this occuring, making it only a minor problem, but 
something that should be addressed nonetheless. 

# Exploratory Analysis of 311 Service Requests

## Overlay Locations of Complaints onto a Map of NYC

To properly achieve this graphic, we will need to leverage python package:
contextily. We will grab an appropriate map of NYC and use each 311 Request's
longitude and latitude to plot each point. 

```{python}

# Import necessary packages
import contextily as ctx
import matplotlib.pyplot as plt

# Make sure coordinates are in numeric
nyc_flood_feather['longitude'] = pd.to_numeric(nyc_flood_feather['longitude'], errors = 'coerce')
nyc_flood_feather['latitude'] = pd.to_numeric(nyc_flood_feather['latitude'], errors = 'coerce')

# Initialize plot
fig, ax = plt.subplots(figsize=(10,10))

# Create scatterplot with longitude along the x, latitude along the y
ax.scatter(nyc_flood_feather['longitude'], nyc_flood_feather['latitude'], color='blue', alpha=0.5, s=10, label='311 Service Complaints')

# Label axis and title graph
plt.xlabel('Longitude', fontdict={'family':'serif', 'color':'darkred','size': 20})
plt.ylabel('Latitude', fontdict={'family':'serif', 'color':'darkred', 'size':20})
plt.title('Map of 311 Service Complaints: Flooding', fontdict={'family':'serif', 'color': 'darkred', 'size':30})

# Overlay map of NYC over the plot
ctx.add_basemap(ax, crs='EPSG:4326', source=ctx.providers.OpenStreetMap.Mapnik)
plt.show()

```


## Creating 'response_time' Variable

I will now create a new variable and house it in the 'response_time' column. 
This column will track the time it takes for the 311 Service Complaint to get
resolved. This column will house numerical values. 

```{python}

# Create new column for the difference between the two times
nyc_flood_feather['response_time'] = nyc_flood_feather['closed_date'] - nyc_flood_feather['created_date']

print(nyc_flood_feather['response_time'].head())

```


## Visualize Reponse Times by Complaint Descriptor and Borough

We will use several histograms for this visualization. We will have 10 
histograms for each of the five boroughs and for the two types of complaint
descriptors, Street Flooding (SJ) and Catch Basin Clogging (SC).  


### Visualizing the Distribution of Response Times

To be able to create accurate and informative histograms, I will create a graph
that will show the distribution of response time in hours. Depending on what
it looks like, I will drop certain outliers and adjust the bins of the 
histograms. 

```{python}

# Import necessary packages
from plotnine import geom_boxplot, ggplot, aes, labs, theme_minimal

# Convert response_time from datetime to num
nyc_flood_feather['response_time'] = pd.to_timedelta(nyc_flood_feather['response_time'])

# Create new column that tracks respone time in hours
nyc_flood_feather['response_time_hours'] = nyc_flood_feather['response_time'].dt.total_seconds() / 3600

boxplot = (
    ggplot(nyc_flood_feather, aes(x='borough', y='response_time_hours'))
    + geom_boxplot(fill='blue', alpha=0.7) 
    + labs(
        title='Distribution of Response Times: Boxplot',
        x='Borough',
        y='Response Time (Hours)'
    )
    + theme_minimal()
)

print(boxplot)

```

From the boxplots we can immediately point out a key insight. Every borough,
besides Queens, has response times within 2,000 hours. Queens has hundreds of 
311 requests that took more that 4,000 hours to be settled, with some taking 
over 6,000 hours to resolve. Whether or not those 311 requests actually took 
years to be solved, these outliers would serve no purpose other than to throw 
off the normality in our dataset, thus they will be removed. 

```{python}

print(f'Data set length before outliers are dropped: {len(nyc_flood_feather)}')

# Drop the rows where response time > 4000 hours
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['response_time_hours'] < 4000]

# Verify Actions
print(f'Data set length after outliers are dropped: {len(nyc_flood_feather)}')

```


### Split the Dataset By Their Descriptors

First step in creating the histograms is splitting the data by their complaint
descriptor. The two types of complaints are 'Street Flooding (SJ)' and 
'Catch Basin Clogged/Flooding'. Splitting the data into two groups will not only
help us create the histograms, but will also help us conduct tests on possible
inequalities in complaints.

```{python}

# Find the exact complaints
print(nyc_flood_feather['descriptor'].value_counts())

# Set descriptor to respective variable
flooding = 'Street Flooding (SJ)'
basin = 'Catch Basin Clogged/Flooding (Use Comments) (SC)'

# Split the data by their complaint descriptor
nyc_street_floods = nyc_flood_feather[nyc_flood_feather['descriptor'] == flooding]
nyc_basin_floods = nyc_flood_feather[nyc_flood_feather['descriptor'] == basin]

# Validate that the dataset properly split
print(nyc_street_floods.head())
print(nyc_basin_floods.head())

```


### Creating Histograms of Each Borough Service Request Response Times

To see if there are differences in response times for the two types of 311 
complaints, we will create two sets of histograms that will visualize the 
response times for every 311 complaint in each borough. We will use the data
sets created in the prior cell that house all 311 complaints from their
respective descriptors. 

```{python}

# Import necessary packages
from plotnine import geom_histogram, facet_wrap

# Create the first histogram for street flooding complaints
sf_histogram = (
    ggplot(nyc_street_floods, aes(x='response_time_hours'))
    + geom_histogram(binwidth=6, fill='blue', color='white', alpha=0.5)
    + labs(
        title='Response Times for Street Flooding by Borough',
        x='Response Time (hours)',
        y='Number of 311 Service Requests'
    )
    + facet_wrap('borough')
    + theme_minimal()
)

# Create the second histogram for basin flooding complaints
bf_histogram = (
    ggplot(nyc_basin_floods, aes(x='response_time_hours'))
    + geom_histogram(binwidth=6, fill='blue', color='white', alpha=0.5)
    + labs(
        title='Response Times for Basin Floods by Borough',
        x='Response Time (hours)',
        y='Number of 311 Service Requests'
    )
    + facet_wrap('borough')
    + theme_minimal()
)

# Print both histograms out and label clearly
print(sf_histogram)
print(bf_histogram)

```


## Differences in Response Times For Specific Complaint Types

From the two sets of histograms, we can see noticable trends inside each 
borough for both types of 311 complaints. Of the bat there is a clear positive 
correlation between the frequency of both complaints. If a borough shows 
evidence of being prone to street flooding, there will be as many problems 
with basin flooding. Lets look at each borough and see what insights we can 
assertain. 


### Bronx

The Bronx is one of the boroughs with the least amount of 311 complaints along
with Manhattan. There are very few street flooding complaints, but as they
occur, they are addressed quickly. There is no tail to the Street Flooding
histogram meaning all of the response times are rather short. Evidence shows
that basin flooding occurances are slightly more common than street flooding. 
There is a noticable tail leading to the right which signals a small problem 
of dealing with these complaints in a timely manner. 


### Brooklyn

Brooklyn is one of the more busy boroughs with a high frequency in both 
complaint types. There are extremely tall bins at the beginning of each 
histogram which is a great sign. Despite the high frequency of 311 complaints, 
the vast majority are addressed in a timely manner. There isn't a noticable 
tail forming in either histogram indicating competance in addressing 311 
complaints. 


### Manhattan

Manhattan has the lowest frequency of both types of 311 complaints. There is an 
extremely low amount of street flooding complaints in Manhattan which indicates
overall good plumbing and drainage in the borough. There are however certainly 
more basin flooding occurances. One tiny detail I noticed while looking at the
basin flooding histogram was the bumpy nature of the histogram. There isn't a 
large concentration of basin flooding complaints but a pattern of several 311
complaints having extremely long response times. For a sample size this small,
it is an indication of ineptitude of addressing basin flooding occurances in a 
timely manner. 


### Queens

Queens has the most 311 complaints for both categories out of all of the 
boroughs. Like Brookyln, Queens has a concentration of 311 complaints at the
left side of the graph. This is a good sign that most 311 complaints get 
addressed quickly. But there are noticable tails to these distributions which 
isn't something you want to see. I believe that due to Queens being the largest
borough in size contributes to this trend. It's hard to address that many 311 
complaints, in such a large landscape, in a quickly manner. 


### Staten Island

For Staten Island's size, they certainly rack up 311 complaints. The histograms
show that they have a sizable number of complaints compared to the Bronx and 
Manhattan, two boroughs that are comparable in size to Staten Island. Staten 
Island generally has quick response times to street flooding occurances, as 
there is no noticable tail in the distribution, but the same cannot be said for
basin floods. There is a near 45 degree slope to the distribution of response
times for basin flooding. An optimal shape for these distributions would be a 
tall peak at the beginning of the distribution with a sharp and sudden drop as
you go towards the right. Statan Island has a clear problem addressing basin
floods in a timely manner. 


## Creating Variable 'over3d' 

The reason for creating this variable is to track the amount of 311 complaints
that have response times over three days long. 


```{python}

# Create new column called over3d
nyc_flood_feather.loc[:, 'over3d'] = nyc_flood_feather['response_time_hours'] > 72

# Convert column from boolean to int.
nyc_flood_feather.loc[:,'over3d'] = nyc_flood_feather['over3d'].astype(int)

# Check that code generate proper column
print(nyc_flood_feather[['response_time_hours', 'over3d']].head())

```

## Reponse Times over 3 Days Long by Complaint Descriptor and Borough

To understand this variable better we will use the same procedure when we 
compared response times for both 311 complaint types in each borough. 

```{python}


# Create new dataframes for the two datasets with response times over 3 days
over3d_street_floods = nyc_street_floods[nyc_street_floods['response_time_hours'] > 72]
over3d_basin_floods = nyc_basin_floods[nyc_basin_floods['response_time_hours'] > 72]

over3d_sf_histogram = (
    ggplot(over3d_street_floods, aes(x='response_time_hours'))
    + geom_histogram(binwidth=3, fill='blue', color='white', alpha=0.5)
    + labs(
        title='Response Times for Street Flooding by Borough',
        x='Response Time (hours)',
        y='Number of 311 Service Requests'
    )
    + facet_wrap('borough')
    + theme_minimal()
)

# Create the second histogram for basin flooding complaints
over3d_bf_histogram = (
    ggplot(over3d_basin_floods, aes(x='response_time_hours'))
    + geom_histogram(binwidth=3, fill='blue', color='white', alpha=0.5)
    + labs(
        title='Response Times for Basin Floods by Borough',
        x='Response Time (hours)',
        y='Number of 311 Service Requests'
    )
    + facet_wrap('borough')
    + theme_minimal()
)

print('Histogram for street flood complaints over three days long')
print(over3d_sf_histogram)
print('Histogram for basin flood complaints over three days long')
print(over3d_bf_histogram)

```


### Interpreting Data and Testing Hypothesis

Before looking at the histograms, I believe that the same patterns will 
present themselves.


## Fitting Logistical Model with 'over3d' As Predictor Variable



```{python}

# Lets create a few new variables that could help us improve our model
# First lets create a column that only includes the hour in which the service
# complaint was created
nyc_flood_feather.loc[:, 'request_hour'] = nyc_flood_feather['created_date'].dt.hour

# Next lets create a variable that indicates if complaint was made during rush hour
nyc_flood_feather.loc[:,'rush_hour'] = ((nyc_flood_feather['request_hour'] >= 7) & (nyc_flood_feather['request_hour'] <= 9)) | ((nyc_flood_feather['request_hour'] >= 16) & (nyc_flood_feather['request_hour'] <= 19))

# Convert from boolean to int
nyc_flood_feather['rush_hour'] = nyc_flood_feather['rush_hour'].astype(int)


# Next we will create a variable to track whether complaint was created during
# the weekend
# Initialize the column to false
nyc_flood_feather.loc[:, 'weekend'] = False

# If the date is on a Saturday or Sunday the row turns to true
nyc_flood_feather.loc[nyc_flood_feather['created_date'].dt.weekday >= 5, 'weekend'] = True

# Convert the column from boolean to int.
nyc_flood_feather['weekend'] = nyc_flood_feather['weekend'].astype(int)


```