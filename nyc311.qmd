---
title: 'Stat 3255 Midterm Project: Analysis of NYC Open Data of 311 Service Requests'
author: 'Ethan Long'
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    documentclass: article
    number-sections: true
    toc: true
    geometry:
      - top=30mm
      - left=20mm
    html:
      code-fold: true
      html-math-method: katex
---

# Introduction

The midterm project of Stat-3255 tasks us with telling a story with NYC Open 
Data 311 Service Requests. More specficially Street Flooding (SF) and Catch 
Basin (CB) complaints. SF complaints indicates instances of street flooding, 
while BC complaints indicate instances of catch basin drains failing to 
properly drain. This could be caused by several reasons, most commonly due to 
drain blockages.



# Data Cleaning 


## Importing File and Getting Familiar With Dataset

To start we must properly import our data file and clean where necessary.
We will do this in several steps. 

Import Data
```{python}

# Import necessary packages
import pandas as pd
import numpy as np
import os 

# Initialize file path to variable 'file'
csv_file = 'nycflood2024.csv'

# Read csv file
nyc_flood = pd.read_csv(csv_file)

# Print first 5 rows, get familiar with dataframe
print(nyc_flood.head())

# Take all column names and properly format
nyc_flood.columns = nyc_flood.columns.str.replace(' ','_').str.lower()

# Summarize missing data in columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

```


##  Converting File Type in Arrow Format for Storage Optimization

We have imported a rather large .csv file and I wish to convert it into a more
efficient file type. Arrow provides us with exactly that, so I will create a
new filepath and convert the .csv file into feather format. I will show the 
amount of space we will save while using the feather file instead of the .csv.

```{python}

# Convert data into arrow format
feather_file = 'nyc_flood_feather.feather'
nyc_flood.to_feather(feather_file)
nyc_flood_feather = pd.read_feather(feather_file)

# See difference in sizes between feather and csv files
print(f'CSV file size: {os.path.getsize(csv_file) / (1024 * 1024)} MB')
print(f'Feather file size: {os.path.getsize(feather_file) / (1024 * 1024)} MB')

```


## Finding and Dropping Redundant Columns

At first glance of the dataframe preview, we can see countless columns with 
either suspicous amounts of missing data or redundant natures. We will explore
numerous columns and see if they are needed for our purposes. 

```{python}

# Find redundant columns

# Noticed how the first five entries in agency_name and complaint 
# type are identical. Tested to see if the entire column is the 'Department of 
# Environmental Protection' and 'Sewer' as complaint type


print(nyc_flood_feather['agency_name'].value_counts())
print(nyc_flood_feather['complaint_type'].value_counts())
# Every entry is identical, no need to keep the columns

# Identify any empty columns
for x in nyc_flood.columns:
    print(f'There are {nyc_flood[x].isnull().sum()} missing values in {x}')

# The following columns are completely empty: 
# location_type, landmark, facility_type, due_date, vehicle_type,
# taxi_company_borough, taxi_pick_up_location, 
# bridge_highway_name, bridge_highway_direction, road_ramp, 
# bridge_highway_segment

# From my prior experience with NYC Open Data, the 'location' is 
# redundant as it contains longitude and latitude coordinates which
# has already been given

# Drop all columns deemed unnecessary
nyc_flood_feather.drop(columns=['agency_name', 'complaint_type', 'location_type', 'landmark', 'facility_type', 'due_date', 'vehicle_type', 'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment'], inplace=True)

# Get summary of new data
print(nyc_flood_feather.head())

```


## Finding Invalid Values of NYC Zipcodes or Boroughs


### Familiarize With the Two Columns in Question

If we are to validate and clean these two columns, we must get comfortable with
both's data and nature. Lets take a preview of the two columns and get a rough
idea what kind of data these columns house.

```{python}

# Familiarize myself with the values in both columns 
print(nyc_flood_feather['incident_zip'].head())
print(nyc_flood_feather['borough'].head())

# Find the frequency of values appearing in borough column
print(nyc_flood_feather['borough'].value_counts())

# Find the amount of incidents missing zip
print(nyc_flood_feather['borough'].value_counts())

```


### Drop the Rows in the Two Columns With Messy Entries

Using our recently gained insights on the contents of both the borough and 
zip code columns, we will drop the rows that contain data we don't want. 

```{python}

# Borough column looks clean besides 2 rows missing
# Zip data looks clean as only 4 are missing entries
# Extremely small sample, we can just drop the 6 total rows
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['borough'] != 'Unspecified']
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].notnull()]

# Double check that the two columns are clean
print(nyc_flood_feather['borough'].value_counts())
print(nyc_flood_feather['incident_zip'].isnull().sum())

```


### Check Zip Codes For Their Validity

We need to check each zip code's validity as our prior experience with NYC's 
Open Data has exposed a slight tendancy to insert invalid zip codes throughout
the data. This most likely occurs through human error and is relatively 
harmless. But as I anticipate myself heavily using zip code data later in this
project, I will drop any row housing an invalid NYC zip code. 


```{python}

# Get all valid nyc zip codes
valid_nyc_zcs = [
    10001, 10002, 10003, 10004, 10005, 10006, 10007, 10009, 10010, 10011, 
    10012, 10013, 10014, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 
    10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 
    10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10044, 10128, 
    10280, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 
    10310, 10312, 10314, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 
    10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 
    10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 11004, 11005, 
    11101, 11102, 11103, 11104, 11105, 11106, 11201, 11203, 11204, 11205, 
    11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 
    11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 
    11226, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 
    11237, 11238, 11239, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 
    11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 
    11372, 11373, 11374, 11375, 11377, 11378, 11379, 11385, 11411, 11412, 
    11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 
    11423, 11426, 11427, 11428, 11429, 11432, 11433, 11434, 11435, 11436, 
    11691, 11692, 11693, 11694, 11695, 11697
]

# Set value of initial list length
initial_len = len(nyc_flood_feather)
print(f'Rows before checking validity of zips: {len(nyc_flood_feather)}')

# Set dataframe to itself but omit all rows with missing zips
nyc_flood_feather = nyc_flood_feather[nyc_flood_feather['incident_zip'].isin(valid_nyc_zcs)]

# Set value of post-parsing list length
post_len = len(nyc_flood_feather)
print(f'Rows after checking validity of zips: {len(nyc_flood_feather)}')

# Show the amount of rows dropped
print(f' There were {initial_len - post_len} rows dropped')

```

## Checking for Possible Dating Errors

The possible dating errors we need to check for in this dataset are as follows:

1. If the 'closed_date' occurs before the 'created_date'
1. If the 'closed_date' occurs at the same time as 'created_date'
1. If the two dates occur at exactly midnight
1. If the two dates occur at exactly noon

```{python}

# Convert the columns to datetime format for efficiency
nyc_flood_feather['closed_date'] = pd.to_datetime(nyc_flood_feather['closed_date'])
nyc_flood_feather['created_date'] = pd.to_datetime(nyc_flood_feather['created_date'])

# Since we are comparing times to noon & midnight, lets define the two times
noon = pd.to_datetime('00:00:00').time()
midnight = pd.to_datetime('12:00:00').time()

# First check rows where closed_date is earlier than created_date
dating_error_count = (
    (nyc_flood_feather['closed_date'] < nyc_flood_feather['created_date']) | 
    # Then check rows where the times are exactly the same
    (nyc_flood_feather['closed_date'] == nyc_flood_feather['created_date']) |
    ( # Then find when the times occur exactly at midnight
        (nyc_flood_feather['closed_date'].dt.time == midnight) & 
        (nyc_flood_feather['created_date'].dt.time == midnight)
    ) | 
    ( # Then find when the times occur exactly at noon
        (nyc_flood_feather['closed_date'].dt.time == noon) & 
        (nyc_flood_feather['created_date'].dt.time == noon)
    )
).sum()

print(f'Number of dating errors found in the dataset: {dating_error_count}')

```

### Dropping Rows Featuring Date Errors

Since there are 159 rows with dating errors, I believe that the best course of 
action is to drop the rows. The dataset spans over 9,000 rows long and I don't
believe dropping 1.7% of the dataset will significantly impact our findings.

```{python}

rows_to_remove = (
      (nyc_flood_feather['closed_date'] < nyc_flood_feather['created_date']) | 
    # Then check rows where the times are exactly the same
    (nyc_flood_feather['closed_date'] == nyc_flood_feather['created_date']) |
    ( # Then find when the times occur exactly at midnight
        (nyc_flood_feather['closed_date'].dt.time == midnight) & 
        (nyc_flood_feather['created_date'].dt.time == midnight)
    ) | 
    ( # Then find when the times occur exactly at noon
        (nyc_flood_feather['closed_date'].dt.time == noon) & 
        (nyc_flood_feather['created_date'].dt.time == noon)
    )
)

nyc_flood_feather = nyc_flood_feather[~rows_to_remove]
print(f' Dataset length before data cleaning: {len(nyc_flood)}')
print(f' Dataset length after data cleaning: {len(nyc_flood_feather)}')

```

## Suggestions to the Data Curator 

From my experience working with the 311 Service Request dataset, I can conclude
that the dataset is a thoughtful, well-rounded dataset that has no major flaws. 
But I believe there are minor tweaks that can be made to improve and optimize
the dataset. To begin there are several columns that are completely empty. 
I don't know whether or not these columns are useful, but I do know that 
they are completely empty. They take up valuable storage space and could 
unneccesarily slow down computing speed. There are 11 of these columns that are
completely empty and therefore obsolete. Secondly there are two columns that 
feature the same entry for all ~9,000 rows. Another example of columns taking 
up storage space and not offering any valuable insights. These two rows,
agency_name & complaint_type, should be omitted in future datasets, unless they
offer variation. Lastly, there are a couple of instances where rows feature
invalid NYC zip codes, empty borough entries, and dating errors. There are only
around ~150 instance of this occuring, making it only a minor problem, but 
something that should be addressed nonetheless. 